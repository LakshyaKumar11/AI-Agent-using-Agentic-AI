{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Building an AI Agent using Agentic AI: Getting Started**\n",
        "Now, let’s get started with building an AI Agent for trading using Agentic AI. I’ll first import all the necessary Python libraries and collect Apple’s stock market data from Yahoo Finance from 1 jan 2025 to 1 july 2025."
      ],
      "metadata": {
        "id": "7-RsQ5PYiDi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60qVM2QS0RbT",
        "outputId": "e2625606-674a-48ff-cb67-c613b6a3e42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-22-130872386.py:16: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(symbol, start=start_date, end=end_date)\n",
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# define stock symbol and time period\n",
        "symbol = \"AAPL\"\n",
        "start_date = \"2025-01-01\"\n",
        "end_date = \"2025-07-01\"\n",
        "\n",
        "# download historical data\n",
        "data = yf.download(symbol, start=start_date, end=end_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, we will calculate technical indicators that help the AI agent make better trading decisions:"
      ],
      "metadata": {
        "id": "FXSkSzEmijUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering\n",
        "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
        "data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
        "data['Returns'] = data['Close'].pct_change()"
      ],
      "metadata": {
        "id": "agHnEQfj1FYm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, let’s drop missing values and reset the index:"
      ],
      "metadata": {
        "id": "Bkb6sK49iuXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop NaN values and reset index\n",
        "data.dropna(inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "ZzRMv0ea1Iqs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## we will define the action space. The AI agent has three possible actions:\n",
        "\n",
        "### HOLD: Do nothing.\n",
        "\n",
        "### BUY: Purchase stocks.\n",
        "\n",
        "### SELL: Sell held stocks."
      ],
      "metadata": {
        "id": "5sppAu7vi6Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define action space\n",
        "ACTIONS = {0: \"HOLD\", 1: \"BUY\", 2: \"SELL\"}"
      ],
      "metadata": {
        "id": "aWldl2151MAF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This action space is used to train the reinforcement learning model."
      ],
      "metadata": {
        "id": "-iPTqUczjOdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get state function\n",
        "def get_state(data, index):\n",
        "    return np.array([\n",
        "        float(data.loc[index, 'Close']),\n",
        "        float(data.loc[index, 'SMA_5']),\n",
        "        float(data.loc[index, 'SMA_20']),\n",
        "        float(data.loc[index, 'Returns'])\n",
        "    ])"
      ],
      "metadata": {
        "id": "18Hc7t-y1Si2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building The Trading Environment for our AI Agent\n",
        "### We will now define a trading environment to interact with the Deep Q-Network (DQN) AI agent, which will allow it to learn how to trade stocks profitably:"
      ],
      "metadata": {
        "id": "MbxkCweXjboI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trading environment\n",
        "class TradingEnvironment:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.initial_balance = 10000\n",
        "        self.balance = self.initial_balance\n",
        "        self.holdings = 0\n",
        "        self.index = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.balance = self.initial_balance\n",
        "        self.holdings = 0\n",
        "        self.index = 0\n",
        "        return get_state(self.data, self.index)\n",
        "\n",
        "    def step(self, action):\n",
        "        price = float(self.data.loc[self.index, 'Close'])\n",
        "        reward = 0\n",
        "\n",
        "        if action == 1 and self.balance >= price:  # BUY\n",
        "            self.holdings = self.balance // price\n",
        "            self.balance -= self.holdings * price\n",
        "        elif action == 2 and self.holdings > 0:  # SELL\n",
        "            self.balance += self.holdings * price\n",
        "            self.holdings = 0\n",
        "\n",
        "        self.index += 1\n",
        "        done = self.index >= len(self.data) - 1\n",
        "\n",
        "        if done:\n",
        "            reward = self.balance - self.initial_balance\n",
        "\n",
        "        next_state = get_state(self.data, self.index) if not done else None\n",
        "        return next_state, reward, done, {}"
      ],
      "metadata": {
        "id": "E3snkuVU1YD5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Deep Q-Network (DQN)\n",
        "### DQN is a neural network that approximates the Q-values for each state-action pair. We will now define the neural network architecture for our Deep Q-Network. It will be responsible for predicting the best trading actions based on the stock market state:"
      ],
      "metadata": {
        "id": "BAhTe3MDjsHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deep q-network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "vc3l_yXe1eOf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here we built a Deep Q-Network using PyTorch to optimize stock trading decisions. The model features a three-layer neural network to predict trading actions, leveraging ReLU activation to enhance learning efficiency.\n",
        "\n",
        "\n",
        "### It outputs Q-values, which the agent utilizes to determine the best action: buy, sell, or hold, based on market conditions.\n",
        "\n",
        "# The DQN Agent\n",
        "### Now, we will implement the AI agent that learns how to trade stocks using Deep Q-Learning. The DQN Agent will interact with the trading environment, make trading decisions (BUY, SELL, HOLD), store experiences, and learn from past experiences to improve future decisions:"
      ],
      "metadata": {
        "id": "PgERr3-Nj36w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95  # Discount factor\n",
        "        self.epsilon = 1.0  # Exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = DQN(state_size, action_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(list(ACTIONS.keys()))\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "                target += self.gamma * torch.max(self.model(next_state_tensor)).item()\n",
        "\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            target_tensor = self.model(state_tensor).clone().detach()\n",
        "            target_tensor[0][action] = target\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(state_tensor)\n",
        "            loss = self.criterion(output, target_tensor)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "tlcGwPx61xIc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So, we developed a Deep Q-Learning Agent to interact with the stock market environment to enhance its decision-making through Experience Replay, which stores and reuses past experiences for training. The agent effectively balances Exploration vs. Exploitation, taking random actions initially and making smarter decisions as learning progresses.\n",
        "\n",
        "### Training is performed using batches of past experiences to refine the neural network’s performance. Additionally, a discount factor (gamma) is applied to weigh immediate and future rewards, to ensure long-term profitability.\n",
        "\n",
        "# Training the AI Agent\n",
        "### Training involves running multiple episodes where the agent interacts with the environment, learns from experience, and updates its model. Let’s train the agent:"
      ],
      "metadata": {
        "id": "DCWhp1ttkGRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the agent\n",
        "env = TradingEnvironment(data)\n",
        "agent = DQNAgent(state_size=4, action_size=3)\n",
        "batch_size = 32\n",
        "episodes = 100\n",
        "total_rewards = []\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    agent.replay(batch_size)\n",
        "    total_rewards.append(total_reward)\n",
        "    print(f\"Episode {episode+1}/{episodes}, Total Reward: {total_reward}\")\n",
        "\n",
        "print(\"Training Complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR7yPsoJ2fN-",
        "outputId": "be787312-e770-4298-d3f9-4fcdda0d9fd3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-26-3109401411.py:4: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  float(data.loc[index, 'Close']),\n",
            "/tmp/ipython-input-26-3109401411.py:5: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  float(data.loc[index, 'SMA_5']),\n",
            "/tmp/ipython-input-26-3109401411.py:6: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  float(data.loc[index, 'SMA_20']),\n",
            "/tmp/ipython-input-26-3109401411.py:7: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  float(data.loc[index, 'Returns'])\n",
            "/tmp/ipython-input-27-2218621516.py:17: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  price = float(self.data.loc[self.index, 'Close'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/100, Total Reward: -1221.8794250488281\n",
            "Episode 2/100, Total Reward: -9804.147079467773\n",
            "Episode 3/100, Total Reward: -9978.898422241211\n",
            "Episode 4/100, Total Reward: -1210.4458923339844\n",
            "Episode 5/100, Total Reward: -9727.65950012207\n",
            "Episode 6/100, Total Reward: -9952.005126953125\n",
            "Episode 7/100, Total Reward: -2549.718963623047\n",
            "Episode 8/100, Total Reward: 1399.851089477539\n",
            "Episode 9/100, Total Reward: -9827.859161376953\n",
            "Episode 10/100, Total Reward: 632.1727752685547\n",
            "Episode 11/100, Total Reward: -375.26513671875\n",
            "Episode 12/100, Total Reward: -393.24151611328125\n",
            "Episode 13/100, Total Reward: -1375.8063049316406\n",
            "Episode 14/100, Total Reward: -9800.107498168945\n",
            "Episode 15/100, Total Reward: 360.84912109375\n",
            "Episode 16/100, Total Reward: -1489.5228271484375\n",
            "Episode 17/100, Total Reward: -2190.211654663086\n",
            "Episode 18/100, Total Reward: -9825.322372436523\n",
            "Episode 19/100, Total Reward: -9801.447998046875\n",
            "Episode 20/100, Total Reward: -9945.72932434082\n",
            "Episode 21/100, Total Reward: -311.7119903564453\n",
            "Episode 22/100, Total Reward: -762.4678039550781\n",
            "Episode 23/100, Total Reward: -9962.061386108398\n",
            "Episode 24/100, Total Reward: -9971.872283935547\n",
            "Episode 25/100, Total Reward: 815.6733551025391\n",
            "Episode 26/100, Total Reward: -1700.6544189453125\n",
            "Episode 27/100, Total Reward: 768.4058990478516\n",
            "Episode 28/100, Total Reward: -710.5437774658203\n",
            "Episode 29/100, Total Reward: -1471.250015258789\n",
            "Episode 30/100, Total Reward: -9978.496307373047\n",
            "Episode 31/100, Total Reward: -775.0105743408203\n",
            "Episode 32/100, Total Reward: -1548.210205078125\n",
            "Episode 33/100, Total Reward: -9882.108383178711\n",
            "Episode 34/100, Total Reward: -9957.201263427734\n",
            "Episode 35/100, Total Reward: -9813.875396728516\n",
            "Episode 36/100, Total Reward: -9799.578170776367\n",
            "Episode 37/100, Total Reward: -1698.379409790039\n",
            "Episode 38/100, Total Reward: -9998.604888916016\n",
            "Episode 39/100, Total Reward: -468.9918212890625\n",
            "Episode 40/100, Total Reward: -9958.241287231445\n",
            "Episode 41/100, Total Reward: -9965.138900756836\n",
            "Episode 42/100, Total Reward: 80.65989685058594\n",
            "Episode 43/100, Total Reward: 595.0035705566406\n",
            "Episode 44/100, Total Reward: -9985.539489746094\n",
            "Episode 45/100, Total Reward: 62.249176025390625\n",
            "Episode 46/100, Total Reward: -9784.511260986328\n",
            "Episode 47/100, Total Reward: -9959.970352172852\n",
            "Episode 48/100, Total Reward: -9802.706558227539\n",
            "Episode 49/100, Total Reward: -9851.43099975586\n",
            "Episode 50/100, Total Reward: -9781.535293579102\n",
            "Episode 51/100, Total Reward: -9862.174301147461\n",
            "Episode 52/100, Total Reward: -633.2935028076172\n",
            "Episode 53/100, Total Reward: -9994.420715332031\n",
            "Episode 54/100, Total Reward: -9970.704055786133\n",
            "Episode 55/100, Total Reward: -1526.100357055664\n",
            "Episode 56/100, Total Reward: -9826.280502319336\n",
            "Episode 57/100, Total Reward: -3213.67724609375\n",
            "Episode 58/100, Total Reward: -9981.880737304688\n",
            "Episode 59/100, Total Reward: -9989.557708740234\n",
            "Episode 60/100, Total Reward: 521.0666656494141\n",
            "Episode 61/100, Total Reward: -9978.103958129883\n",
            "Episode 62/100, Total Reward: -141.32284545898438\n",
            "Episode 63/100, Total Reward: -9937.950012207031\n",
            "Episode 64/100, Total Reward: -9794.844268798828\n",
            "Episode 65/100, Total Reward: 791.5670318603516\n",
            "Episode 66/100, Total Reward: 478.03733825683594\n",
            "Episode 67/100, Total Reward: 637.8561859130859\n",
            "Episode 68/100, Total Reward: -9845.19270324707\n",
            "Episode 69/100, Total Reward: -2587.786392211914\n",
            "Episode 70/100, Total Reward: -9718.483016967773\n",
            "Episode 71/100, Total Reward: -9986.199584960938\n",
            "Episode 72/100, Total Reward: -9943.044967651367\n",
            "Episode 73/100, Total Reward: -9975.76774597168\n",
            "Episode 74/100, Total Reward: -9961.321380615234\n",
            "Episode 75/100, Total Reward: -9983.226577758789\n",
            "Episode 76/100, Total Reward: -9815.455261230469\n",
            "Episode 77/100, Total Reward: -9977.04736328125\n",
            "Episode 78/100, Total Reward: -663.3088073730469\n",
            "Episode 79/100, Total Reward: -9790.561447143555\n",
            "Episode 80/100, Total Reward: 53.213592529296875\n",
            "Episode 81/100, Total Reward: 197.48251342773438\n",
            "Episode 82/100, Total Reward: -9866.656784057617\n",
            "Episode 83/100, Total Reward: -9824.714660644531\n",
            "Episode 84/100, Total Reward: -9866.395538330078\n",
            "Episode 85/100, Total Reward: -723.7718811035156\n",
            "Episode 86/100, Total Reward: 625.6869049072266\n",
            "Episode 87/100, Total Reward: 2823.108871459961\n",
            "Episode 88/100, Total Reward: 3512.0201263427734\n",
            "Episode 89/100, Total Reward: 890.1607666015625\n",
            "Episode 90/100, Total Reward: -507.6693878173828\n",
            "Episode 91/100, Total Reward: -360.83245849609375\n",
            "Episode 92/100, Total Reward: -9851.368194580078\n",
            "Episode 93/100, Total Reward: -1674.7210083007812\n",
            "Episode 94/100, Total Reward: -67.86087036132812\n",
            "Episode 95/100, Total Reward: 373.59791564941406\n",
            "Episode 96/100, Total Reward: -9774.573150634766\n",
            "Episode 97/100, Total Reward: -9957.741271972656\n",
            "Episode 98/100, Total Reward: -9931.53385925293\n",
            "Episode 99/100, Total Reward: -9740.121627807617\n",
            "Episode 100/100, Total Reward: -1241.5285034179688\n",
            "Training Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here, we trained the AI Trading Agent using Deep Q-Learning, simulating 100 trading sessions where the agent learned from experience. It leveraged Exploration & Exploitation, initially taking random actions before making more informed decisions as training progressed.\n",
        "\n",
        "\n",
        "### Experience Replay is used to store past experiences, allowing the neural network to learn through batch training. Throughout the process, we tracked rewards to measure the agent’s performance improvements over time.\n",
        "\n",
        "### After training, we can test the agent on new market data by allowing it to make decisions without random exploration:"
      ],
      "metadata": {
        "id": "6Ems-uqkkUpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a fresh environment instance for testing\n",
        "test_env = TradingEnvironment(data)\n",
        "state = test_env.reset()\n",
        "done = False\n",
        "\n",
        "# simulate a trading session using the trained agent\n",
        "while not done:\n",
        "    # always choose the best action (exploitation)\n",
        "    action = agent.act(state)\n",
        "    next_state, reward, done, _ = test_env.step(action)\n",
        "    state = next_state if next_state is not None else state\n",
        "\n",
        "final_balance = test_env.balance\n",
        "profit = final_balance - test_env.initial_balance\n",
        "print(f\"Final Balance after testing: ${final_balance:.2f}\")\n",
        "print(f\"Total Profit: ${profit:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAHD_Krb8gYl",
        "outputId": "f7a476f4-9dbe-4023-b1c4-ee5704474f77"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Balance after testing: $11441.16\n",
            "Total Profit: $1441.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-26-3109401411.py:4: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  float(data.loc[index, 'Close']),\n",
            "/tmp/ipython-input-26-3109401411.py:5: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  float(data.loc[index, 'SMA_5']),\n",
            "/tmp/ipython-input-26-3109401411.py:6: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  float(data.loc[index, 'SMA_20']),\n",
            "/tmp/ipython-input-26-3109401411.py:7: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  float(data.loc[index, 'Returns'])\n",
            "/tmp/ipython-input-27-2218621516.py:17: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  price = float(self.data.loc[self.index, 'Close'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agent started with $10,000 and ended with $11,441.16. Profit = $1441.16, meaning the agent made a good positive return.\n",
        "\n",
        "So, this is how you can build an AI Agent using Agentic AI."
      ],
      "metadata": {
        "id": "7nW1u5YUka_w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DRdTYNCO8jeY"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}